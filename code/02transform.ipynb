{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first 500,000 rows of the csv as a pandas dataframe\n",
    "\n",
    "file_path = \"../data/raw/PS_20174392719_1491204439457_log.csv\"\n",
    "df_sample = pd.read_csv(file_path, nrows=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step              0\n",
       "type              0\n",
       "amount            0\n",
       "nameOrig          0\n",
       "oldbalanceOrg     0\n",
       "newbalanceOrig    0\n",
       "nameDest          0\n",
       "oldbalanceDest    0\n",
       "newbalanceDest    0\n",
       "isFraud           0\n",
       "isFlaggedFraud    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite not having any null values, the previous method call does give us the names of all columns.\n",
    "\n",
    "During the eda phase, we learned that certain columns didn't give any useful information for analysis (step, nameOrig, nameDest) and certain columns didn't contribute to useful visualizations (newbalanceOrig, oldbalanceDest, newbalanceDest). The visualizations also showed how similar the distributions between oldbalanceOrg and newbalanceOrig, and oldbalanceDest and newbalanceDest are, suggesting some of the columns are redundant, but may still be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [\"step\", \"nameOrig\", \"nameDest\"]\n",
    "sample_drop = df_sample.drop(columns=selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "chunk_size = 5000  # Define your chunk size based on your system\"s memory capacity\n",
    "processed_chunks = []  # List to store processed chunks\n",
    "# Define numerical and categorical features\n",
    "numeric_features = [\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"isFraud\"]\n",
    "categorical_features = [\"type\"]  # Adjust based on your actual categorical features\n",
    "# Define the preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), numeric_features),\n",
    "    (\"cat\", OneHotEncoder(), categorical_features)])\n",
    "# Fit the preprocessor on the sample\n",
    "preprocessor.fit(df_sample)\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Apply the preprocessing to each chunk\n",
    "    X_chunk = chunk.drop(\"isFraud\",axis=1)\n",
    "    y_chunk = chunk[\"isFraud\"]\n",
    "    processed_X_chunk = preprocessor.transform(chunk)\n",
    "    # Convert the processed chunk back to DataFrame (optional, if you need a DataFrame for further processing)\n",
    "    # Note: Adjust column names in `processed_chunk_df` based on the output of your preprocessor\n",
    "    numeric_features_processed = [f\"{feature}_scaled\" for feature in numeric_features]\n",
    "    categorical_features_encoded = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "    all_features = numeric_features_processed + list(categorical_features_encoded)\n",
    "    processed_chunk_df = pd.DataFrame(processed_X_chunk, columns=all_features)\n",
    "    processed_chunk_df[\"isFraud\"] = y_chunk.reset_index(drop=True)\n",
    "    # Store the processed DataFrame\n",
    "    processed_chunks.append(processed_chunk_df)\n",
    "# Concatenate all processed chunks back into a single DataFrame\n",
    "processed_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "processed_df.drop(\"isFraud_scaled\",axis=1,inplace=True)\n",
    "processed_df.to_csv(\"../data/processed/process_data_chunk.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phase1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
